{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM JP instruction-tuning Finetuning with HuggingFace and Weights and Biases\n",
    "<!--- @wandbcode{llm-finetune-hf} -->\n",
    "- Fine-tune a lightweight LLM with LoRA and 8-bit quantization\n",
    "- Checkpoint the LoRA adapter weights as artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import wandb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeisuke-kamata\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"japan-demo\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"jp-instruction-tuning\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=1024\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"BASE_MODEL\":\"cyberagent/open-calm-3b\",\n",
    "    \"lora_config\":{\n",
    "        \"r\":32,\n",
    "        \"lora_alpha\":16,\n",
    "        \"target_modules\":[\"query_key_value\"],\n",
    "        \"lora_dropout\":.1,\n",
    "        \"bias\":\"none\",\n",
    "        \"task_type\":\"CAUSAL_LM\"\n",
    "    },\n",
    "    \"training_args\":{\n",
    "        \"dataloader_num_workers\":16,\n",
    "        \"evaluation_strategy\":\"steps\",\n",
    "        \"per_device_train_batch_size\":8,\n",
    "        \"max_steps\": 100,\n",
    "        \"gradient_accumulation_steps\":2,\n",
    "        \"report_to\":\"wandb\",#wandb integration\n",
    "        \"warmup_steps\":10,\n",
    "        \"num_train_epochs\":1,\n",
    "        \"learning_rate\":2e-4,\n",
    "        \"fp16\":True,\n",
    "        \"logging_steps\":10,\n",
    "        \"save_steps\":25,\n",
    "        \"output_dir\":'./outputs'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config[\"BASE_MODEL\"],\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"BASE_MODEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_ja = datasets.load_dataset(\"kunishou/databricks-dolly-15k-ja\")\n",
    "dolly_ja = list(dolly_ja['train'])\n",
    "split_index = int(len(dolly_ja) * 0.8)  # 例として80%をトレーニングデータとして使用\n",
    "dolly_ja_train = dolly_ja[:split_index]\n",
    "dolly_ja_val = dolly_ja[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "        \"### Instruction:{instruction} \\n\\n Input:{input} \\n\\n ###Response\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "        \"### Instruction:{instruction} \\n\\n ###Response\"\n",
    "    )\n",
    "}\n",
    "\n",
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, json_list, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = ignore_index\n",
    "        self.features = []\n",
    "        \n",
    "        for j in tqdm(json_list):\n",
    "            if 'input' in j:\n",
    "                source_text = PROMPT_DICT['prompt_input'].format_map(j)\n",
    "            else:\n",
    "                source_text = PROMPT_DICT['prompt_no_input'].format_map(j)\n",
    "            example_text = source_text + j['output'] + self.tokenizer.eos_token\n",
    "            \n",
    "            source_tokenized = self.tokenizer(\n",
    "                source_text,\n",
    "                padding='longest',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_length=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            example_tokenized = self.tokenizer(\n",
    "                example_text, \n",
    "                padding='longest', \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = example_tokenized['input_ids'][0]\n",
    "            labels = copy.deepcopy(input_ids)\n",
    "            source_len = source_tokenized['length'][0]\n",
    "            labels[:source_len] = self.ignore_index\n",
    "            \n",
    "            self.features.append({\n",
    "                'input_ids': input_ids,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "\n",
    "class InstructCollator():\n",
    "    def __init__(self, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = -100\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        input_batch = []\n",
    "        label_batch = []\n",
    "        for example in examples:\n",
    "            input_batch.append(example['input_ids'])\n",
    "            label_batch.append(example['labels'])\n",
    "        input_ids = pad_sequence(\n",
    "            input_batch, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = pad_sequence(\n",
    "            label_batch, batch_first=True, padding_value=self.ignore_index\n",
    "        )\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)   \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12012/12012 [00:06<00:00, 1913.98it/s]\n",
      "100%|██████████| 3003/3003 [00:01<00:00, 1823.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = InstructDataset(dolly_ja_train, tokenizer)\n",
    "val_dataset = InstructDataset(dolly_ja_val, tokenizer)\n",
    "collator = InstructCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの中身を確認\n",
    "print(model.gpt_neox.layers[0].attention)\n",
    "#GPTNeoXAttention(\n",
    "#  (rotary_emb): RotaryEmbedding()\n",
    "#  (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
    "#  (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "\n",
    "# cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float32)\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.embed_out = CastOutputToFloat(model.embed_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(config=config, job_type=\"training\") as run:\n",
    "    # Setup for LoRa\n",
    "    lora_config = LoraConfig(**wandb.config[\"lora_config\"])\n",
    "    model_peft = get_peft_model(model, lora_config)    \n",
    "    model_peft.print_trainable_parameters()\n",
    "    model_peft.config.use_cache = False\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model_peft,\n",
    "        data_collator=collator,\n",
    "        args=transformers.TrainingArguments(**wandb.config[\"training_args\"]),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    model_peft.save_pretrained(\"./output\")\n",
    "    model_ft = wandb.Artifact(f\"finetuned-model\", type=\"model\")\n",
    "    model_ft.add_dir(\"./output\")\n",
    "    run.log_artifact(model_ft)\n",
    "    run.log_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Advanced) Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8e17pj90\n",
      "Sweep URL: https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g63ga8cb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001107952583922356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeisuke-kamata\u001b[0m (\u001b[33mjapan-demo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_145742-g63ga8cb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/g63ga8cb' target=\"_blank\">hearty-sweep-1</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/g63ga8cb' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/g63ga8cb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,310,720 || all params: 2,786,350,080 || trainable%: 0.04704075088798605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.983700</td>\n",
       "      <td>2.822979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.814500</td>\n",
       "      <td>2.642505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.648900</td>\n",
       "      <td>2.593680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.634500</td>\n",
       "      <td>2.574844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.634300</td>\n",
       "      <td>2.560660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.403500</td>\n",
       "      <td>2.553237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.746500</td>\n",
       "      <td>2.548208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.642200</td>\n",
       "      <td>2.544298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.664400</td>\n",
       "      <td>2.542321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.577200</td>\n",
       "      <td>2.541400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▁▃▇▇▄▂▃██</td></tr><tr><td>eval/samples_per_second</td><td>▄█▆▃▃▅▇▆▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▅█▅▁▁▅▅▅▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.5414</td></tr><tr><td>eval/runtime</td><td>120.0953</td></tr><tr><td>eval/samples_per_second</td><td>25.005</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5772</td></tr><tr><td>train/total_flos</td><td>1.015513358303232e+16</td></tr><tr><td>train/train_loss</td><td>2.67496</td></tr><tr><td>train/train_runtime</td><td>1432.8685</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-1</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/g63ga8cb' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/g63ga8cb</a><br/>Synced 6 W&B file(s), 0 media file(s), 45 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_145742-g63ga8cb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7v7v6wva with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009351076911857216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_152221-7v7v6wva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/7v7v6wva' target=\"_blank\">polar-sweep-2</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/7v7v6wva' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/7v7v6wva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,485,760 || all params: 2,795,525,120 || trainable%: 0.37509088811192653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.947500</td>\n",
       "      <td>2.710118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.757700</td>\n",
       "      <td>2.603467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.622600</td>\n",
       "      <td>2.574393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.619500</td>\n",
       "      <td>2.557958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.616900</td>\n",
       "      <td>2.546619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.392900</td>\n",
       "      <td>2.539884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.732500</td>\n",
       "      <td>2.535327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.629900</td>\n",
       "      <td>2.531780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.652800</td>\n",
       "      <td>2.529044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.561400</td>\n",
       "      <td>2.528010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.2s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▂▁█▃▇█▃▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▅▇█▁▆▂▁▆▅▆</td></tr><tr><td>eval/steps_per_second</td><td>▅██▁▅▁▁▅▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.52801</td></tr><tr><td>eval/runtime</td><td>120.1162</td></tr><tr><td>eval/samples_per_second</td><td>25.001</td></tr><tr><td>eval/steps_per_second</td><td>3.13</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5614</td></tr><tr><td>train/total_flos</td><td>1.019025827856384e+16</td></tr><tr><td>train/train_loss</td><td>2.65338</td></tr><tr><td>train/train_runtime</td><td>1434.2408</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-2</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/7v7v6wva' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/7v7v6wva</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_152221-7v7v6wva/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2osa068c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006140942853396663\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_154815-2osa068c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/2osa068c' target=\"_blank\">magic-sweep-3</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/2osa068c' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/2osa068c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,485,760 || all params: 2,795,525,120 || trainable%: 0.37509088811192653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.000600</td>\n",
       "      <td>2.928456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.918500</td>\n",
       "      <td>2.690914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.695900</td>\n",
       "      <td>2.638189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.666900</td>\n",
       "      <td>2.605998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.666800</td>\n",
       "      <td>2.588048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.427500</td>\n",
       "      <td>2.578076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.768900</td>\n",
       "      <td>2.571414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.664100</td>\n",
       "      <td>2.567323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.682300</td>\n",
       "      <td>2.565076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.598600</td>\n",
       "      <td>2.563603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▂▃▄█▁▃▁█▅</td></tr><tr><td>eval/samples_per_second</td><td>▇▇▆▅▁█▅█▁▄</td></tr><tr><td>eval/steps_per_second</td><td>██▆▆▁█▆█▁▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.5636</td></tr><tr><td>eval/runtime</td><td>120.1768</td></tr><tr><td>eval/samples_per_second</td><td>24.988</td></tr><tr><td>eval/steps_per_second</td><td>3.129</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5986</td></tr><tr><td>train/total_flos</td><td>1.019025827856384e+16</td></tr><tr><td>train/train_loss</td><td>2.70902</td></tr><tr><td>train/train_runtime</td><td>1434.365</td></tr><tr><td>train/train_samples_per_second</td><td>1.115</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sweep-3</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/2osa068c' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/2osa068c</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_154815-2osa068c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kl2cjicp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0012960543474547276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_161413-kl2cjicp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/kl2cjicp' target=\"_blank\">misty-sweep-4</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/kl2cjicp' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/kl2cjicp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,310,720 || all params: 2,786,350,080 || trainable%: 0.04704075088798605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.925800</td>\n",
       "      <td>2.680261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.737500</td>\n",
       "      <td>2.592384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.612800</td>\n",
       "      <td>2.573225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.616200</td>\n",
       "      <td>2.552280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.612800</td>\n",
       "      <td>2.541932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.389400</td>\n",
       "      <td>2.534371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.727800</td>\n",
       "      <td>2.530124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>2.526201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.650500</td>\n",
       "      <td>2.523867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.562300</td>\n",
       "      <td>2.523340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▇▇▇▆▆▄▆▁▅█</td></tr><tr><td>eval/samples_per_second</td><td>▁▂▂▃▃▅▃█▄▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▅▅▅▅█▅▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.52334</td></tr><tr><td>eval/runtime</td><td>120.0868</td></tr><tr><td>eval/samples_per_second</td><td>25.007</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5623</td></tr><tr><td>train/total_flos</td><td>1.015513358303232e+16</td></tr><tr><td>train/train_loss</td><td>2.64602</td></tr><tr><td>train/train_runtime</td><td>1432.3734</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-sweep-4</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/kl2cjicp' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/kl2cjicp</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_161413-kl2cjicp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qo94zzt6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007772866904158016\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_163842-qo94zzt6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/qo94zzt6' target=\"_blank\">graceful-sweep-5</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/qo94zzt6' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/qo94zzt6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,310,720 || all params: 2,786,350,080 || trainable%: 0.04704075088798605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.956100</td>\n",
       "      <td>2.734229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.771400</td>\n",
       "      <td>2.612916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.628500</td>\n",
       "      <td>2.579769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.623100</td>\n",
       "      <td>2.563002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.622700</td>\n",
       "      <td>2.550567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.397800</td>\n",
       "      <td>2.543941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.736400</td>\n",
       "      <td>2.539079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.634200</td>\n",
       "      <td>2.535614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.655900</td>\n",
       "      <td>2.533225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.569500</td>\n",
       "      <td>2.532444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▂▆▄█▃▁▂▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▄▆▃▅▁▆█▆▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▅█▅▅▁█████</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.53244</td></tr><tr><td>eval/runtime</td><td>120.1072</td></tr><tr><td>eval/samples_per_second</td><td>25.003</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5695</td></tr><tr><td>train/total_flos</td><td>1.015513358303232e+16</td></tr><tr><td>train/train_loss</td><td>2.65956</td></tr><tr><td>train/train_runtime</td><td>1432.9596</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-5</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/qo94zzt6' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/qo94zzt6</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_163842-qo94zzt6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 11fqwymk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005188422513377607\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_170315-11fqwymk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/11fqwymk' target=\"_blank\">olive-sweep-6</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/11fqwymk' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/11fqwymk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 655,360 || all params: 2,785,694,720 || trainable%: 0.02352590882607553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.002400</td>\n",
       "      <td>2.940428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.941400</td>\n",
       "      <td>2.712111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.710800</td>\n",
       "      <td>2.647865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.676600</td>\n",
       "      <td>2.615963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.675900</td>\n",
       "      <td>2.598461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.440100</td>\n",
       "      <td>2.587663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.775400</td>\n",
       "      <td>2.580106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.673200</td>\n",
       "      <td>2.575370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.694100</td>\n",
       "      <td>2.572934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.606000</td>\n",
       "      <td>2.572294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▅▅▄▃█▁▄▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▄▅▆▁█▅█▆</td></tr><tr><td>eval/steps_per_second</td><td>▅▅▅▅▅▁█▅██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.57229</td></tr><tr><td>eval/runtime</td><td>120.1078</td></tr><tr><td>eval/samples_per_second</td><td>25.003</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.606</td></tr><tr><td>train/total_flos</td><td>1.015262467620864e+16</td></tr><tr><td>train/train_loss</td><td>2.7196</td></tr><tr><td>train/train_runtime</td><td>1433.0621</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-sweep-6</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/11fqwymk' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/11fqwymk</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_170315-11fqwymk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3vf4bcdf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006101233068819079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_172739-3vf4bcdf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/3vf4bcdf' target=\"_blank\">youthful-sweep-7</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/3vf4bcdf' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/3vf4bcdf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 655,360 || all params: 2,785,694,720 || trainable%: 0.02352590882607553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.935900</td>\n",
       "      <td>2.702642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.759600</td>\n",
       "      <td>2.609017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.629900</td>\n",
       "      <td>2.577864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.621500</td>\n",
       "      <td>2.561050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.620800</td>\n",
       "      <td>2.548752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.396200</td>\n",
       "      <td>2.542703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.736500</td>\n",
       "      <td>2.538094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.632400</td>\n",
       "      <td>2.534559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.655900</td>\n",
       "      <td>2.532296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.567600</td>\n",
       "      <td>2.531905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▁▂█▃▃▂▅▂▅</td></tr><tr><td>eval/samples_per_second</td><td>▆█▇▁▇▆▇▄▇▄</td></tr><tr><td>eval/steps_per_second</td><td>▆██▁▆▆█▃█▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.53191</td></tr><tr><td>eval/runtime</td><td>120.1543</td></tr><tr><td>eval/samples_per_second</td><td>24.993</td></tr><tr><td>eval/steps_per_second</td><td>3.129</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5676</td></tr><tr><td>train/total_flos</td><td>1.015262467620864e+16</td></tr><tr><td>train/train_loss</td><td>2.65564</td></tr><tr><td>train/train_runtime</td><td>1433.0711</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-7</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/3vf4bcdf' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/3vf4bcdf</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_172739-3vf4bcdf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yeq43dar with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007143254916769785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_175215-yeq43dar</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/yeq43dar' target=\"_blank\">clean-sweep-8</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/yeq43dar' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/yeq43dar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,485,760 || all params: 2,795,525,120 || trainable%: 0.37509088811192653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.983600</td>\n",
       "      <td>2.831631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.821400</td>\n",
       "      <td>2.646711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.656100</td>\n",
       "      <td>2.597736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.637500</td>\n",
       "      <td>2.577672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.637900</td>\n",
       "      <td>2.562819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.404500</td>\n",
       "      <td>2.556147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.746700</td>\n",
       "      <td>2.550408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.642300</td>\n",
       "      <td>2.546877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.665800</td>\n",
       "      <td>2.544974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.578900</td>\n",
       "      <td>2.544265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▇▃▅█▂▃▄▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▃▂▆▄▁▇▆▅█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▁█▅▁██▅██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.54427</td></tr><tr><td>eval/runtime</td><td>120.1627</td></tr><tr><td>eval/samples_per_second</td><td>24.991</td></tr><tr><td>eval/steps_per_second</td><td>3.129</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5789</td></tr><tr><td>train/total_flos</td><td>1.019025827856384e+16</td></tr><tr><td>train/train_loss</td><td>2.67747</td></tr><tr><td>train/train_runtime</td><td>1435.235</td></tr><tr><td>train/train_samples_per_second</td><td>1.115</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-sweep-8</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/yeq43dar' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/yeq43dar</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_175215-yeq43dar/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t3a4xwvl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0016240452856319432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_181803-t3a4xwvl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/t3a4xwvl' target=\"_blank\">scarlet-sweep-9</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/t3a4xwvl' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/t3a4xwvl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,242,880 || all params: 2,790,282,240 || trainable%: 0.18789783789040637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.913800</td>\n",
       "      <td>2.670833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.724000</td>\n",
       "      <td>2.583606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.604300</td>\n",
       "      <td>2.569542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.612200</td>\n",
       "      <td>2.548271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.607900</td>\n",
       "      <td>2.536728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.382800</td>\n",
       "      <td>2.530697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.724700</td>\n",
       "      <td>2.525887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.620200</td>\n",
       "      <td>2.522869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.645900</td>\n",
       "      <td>2.519764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.559300</td>\n",
       "      <td>2.518792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▆█▄▃▅▄▅▃▃</td></tr><tr><td>eval/samples_per_second</td><td>█▃▁▅▆▄▅▄▆▆</td></tr><tr><td>eval/steps_per_second</td><td>█▁▁▅▅▅▅▅▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▄▄▁▆▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.51879</td></tr><tr><td>eval/runtime</td><td>120.0763</td></tr><tr><td>eval/samples_per_second</td><td>25.009</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5593</td></tr><tr><td>train/total_flos</td><td>1.01701870239744e+16</td></tr><tr><td>train/train_loss</td><td>2.6395</td></tr><tr><td>train/train_runtime</td><td>1433.2587</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-sweep-9</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/t3a4xwvl' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/t3a4xwvl</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_181803-t3a4xwvl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nqz5tl7p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00033230850505124813\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_184310-nqz5tl7p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/nqz5tl7p' target=\"_blank\">tough-sweep-10</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/nqz5tl7p' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/nqz5tl7p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,621,440 || all params: 2,787,660,800 || trainable%: 0.09403726593995941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.993200</td>\n",
       "      <td>2.890548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.878300</td>\n",
       "      <td>2.670422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.681900</td>\n",
       "      <td>2.627282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.657800</td>\n",
       "      <td>2.599611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.657100</td>\n",
       "      <td>2.581806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.422700</td>\n",
       "      <td>2.573025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.764100</td>\n",
       "      <td>2.567003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.658600</td>\n",
       "      <td>2.563356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.680600</td>\n",
       "      <td>2.560772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.595500</td>\n",
       "      <td>2.559992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▅▆█▃▆▃▃▃▅</td></tr><tr><td>eval/samples_per_second</td><td>█▅▄▁▆▃▆▆▆▄</td></tr><tr><td>eval/steps_per_second</td><td>█▄▄▁▄▁█▄█▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.55999</td></tr><tr><td>eval/runtime</td><td>120.1739</td></tr><tr><td>eval/samples_per_second</td><td>24.989</td></tr><tr><td>eval/steps_per_second</td><td>3.129</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5955</td></tr><tr><td>train/total_flos</td><td>1.016015139667968e+16</td></tr><tr><td>train/train_loss</td><td>2.69897</td></tr><tr><td>train/train_runtime</td><td>1433.8873</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-10</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/nqz5tl7p' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/nqz5tl7p</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_184310-nqz5tl7p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gz6rcgzw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003884976318730308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_190755-gz6rcgzw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/gz6rcgzw' target=\"_blank\">toasty-sweep-11</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/gz6rcgzw' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/gz6rcgzw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 655,360 || all params: 2,785,694,720 || trainable%: 0.02352590882607553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.962500</td>\n",
       "      <td>2.766867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.791300</td>\n",
       "      <td>2.637496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.648400</td>\n",
       "      <td>2.593835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.634000</td>\n",
       "      <td>2.575090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.635500</td>\n",
       "      <td>2.561079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.405800</td>\n",
       "      <td>2.554270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.746400</td>\n",
       "      <td>2.548455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.641400</td>\n",
       "      <td>2.544444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.663900</td>\n",
       "      <td>2.542184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.576800</td>\n",
       "      <td>2.541398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂█▃▁▇▂▁▂▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▇▁▆█▂██▇██</td></tr><tr><td>eval/steps_per_second</td><td>▆▁▆█▃██▆██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.5414</td></tr><tr><td>eval/runtime</td><td>120.1052</td></tr><tr><td>eval/samples_per_second</td><td>25.003</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5768</td></tr><tr><td>train/total_flos</td><td>1.015262467620864e+16</td></tr><tr><td>train/train_loss</td><td>2.67059</td></tr><tr><td>train/train_runtime</td><td>1432.9</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-11</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/gz6rcgzw' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/gz6rcgzw</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_190755-gz6rcgzw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j44p4gee with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002745971673760735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_193218-j44p4gee</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/j44p4gee' target=\"_blank\">rosy-sweep-12</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/j44p4gee' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/j44p4gee</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,242,880 || all params: 2,790,282,240 || trainable%: 0.18789783789040637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.007900</td>\n",
       "      <td>2.982340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>2.855689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.839900</td>\n",
       "      <td>2.737135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.745800</td>\n",
       "      <td>2.682171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.735300</td>\n",
       "      <td>2.653193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.492300</td>\n",
       "      <td>2.637036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.824300</td>\n",
       "      <td>2.627062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.716700</td>\n",
       "      <td>2.621220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.733600</td>\n",
       "      <td>2.617736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.651800</td>\n",
       "      <td>2.616482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.1s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▂▁▄▃▇▃█▇▅</td></tr><tr><td>eval/samples_per_second</td><td>▇▇█▅▇▁▆▁▂▄</td></tr><tr><td>eval/steps_per_second</td><td>███▄█▁▄▁▁▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>▇█▅▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.61648</td></tr><tr><td>eval/runtime</td><td>120.1617</td></tr><tr><td>eval/samples_per_second</td><td>24.991</td></tr><tr><td>eval/steps_per_second</td><td>3.129</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.6518</td></tr><tr><td>train/total_flos</td><td>1.01701870239744e+16</td></tr><tr><td>train/train_loss</td><td>2.78016</td></tr><tr><td>train/train_runtime</td><td>1433.8212</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-12</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/j44p4gee' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/j44p4gee</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_193218-j44p4gee/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 77yh6xy3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0013757282456376525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_195723-77yh6xy3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/77yh6xy3' target=\"_blank\">astral-sweep-13</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/77yh6xy3' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/77yh6xy3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,310,720 || all params: 2,786,350,080 || trainable%: 0.04704075088798605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.951500</td>\n",
       "      <td>2.711336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.755800</td>\n",
       "      <td>2.601010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.623200</td>\n",
       "      <td>2.574450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.620600</td>\n",
       "      <td>2.557652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.615800</td>\n",
       "      <td>2.545607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.392600</td>\n",
       "      <td>2.539224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.732100</td>\n",
       "      <td>2.534562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.628600</td>\n",
       "      <td>2.530815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.652900</td>\n",
       "      <td>2.528745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.563200</td>\n",
       "      <td>2.527348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▆▅█▄▄▁▃▂▄</td></tr><tr><td>eval/samples_per_second</td><td>▅▃▄▁▅▅█▆▇▅</td></tr><tr><td>eval/steps_per_second</td><td>▄▃▄▁▄▄█▆▆▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.52735</td></tr><tr><td>eval/runtime</td><td>120.004</td></tr><tr><td>eval/samples_per_second</td><td>25.024</td></tr><tr><td>eval/steps_per_second</td><td>3.133</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5632</td></tr><tr><td>train/total_flos</td><td>1.015513358303232e+16</td></tr><tr><td>train/train_loss</td><td>2.65362</td></tr><tr><td>train/train_runtime</td><td>1432.102</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-sweep-13</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/77yh6xy3' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/77yh6xy3</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_195723-77yh6xy3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jaeyf11w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0010734874956814093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_202159-jaeyf11w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/jaeyf11w' target=\"_blank\">royal-sweep-14</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/jaeyf11w' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/jaeyf11w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 655,360 || all params: 2,785,694,720 || trainable%: 0.02352590882607553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.984100</td>\n",
       "      <td>2.821956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.812800</td>\n",
       "      <td>2.645225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.649900</td>\n",
       "      <td>2.595028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.632300</td>\n",
       "      <td>2.574992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.635800</td>\n",
       "      <td>2.561187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.405600</td>\n",
       "      <td>2.554585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.746500</td>\n",
       "      <td>2.549136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.642300</td>\n",
       "      <td>2.545339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.662900</td>\n",
       "      <td>2.543334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.577500</td>\n",
       "      <td>2.542664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▆▃▄▂█▃▃▁▄</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▅▅▆▁▆▆█▅</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▆▆▆▁▆▆█▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.54266</td></tr><tr><td>eval/runtime</td><td>120.1124</td></tr><tr><td>eval/samples_per_second</td><td>25.002</td></tr><tr><td>eval/steps_per_second</td><td>3.13</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5775</td></tr><tr><td>train/total_flos</td><td>1.015262467620864e+16</td></tr><tr><td>train/train_loss</td><td>2.67496</td></tr><tr><td>train/train_runtime</td><td>1432.9903</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-sweep-14</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/jaeyf11w' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/jaeyf11w</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_202159-jaeyf11w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ccg48k2c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0018511692943030257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_204623-ccg48k2c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/ccg48k2c' target=\"_blank\">chocolate-sweep-15</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/ccg48k2c' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/ccg48k2c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 655,360 || all params: 2,785,694,720 || trainable%: 0.02352590882607553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.930500</td>\n",
       "      <td>2.680303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.737900</td>\n",
       "      <td>2.592973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.615500</td>\n",
       "      <td>2.573115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.617200</td>\n",
       "      <td>2.552472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.612600</td>\n",
       "      <td>2.542547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.390100</td>\n",
       "      <td>2.536153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>2.531480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.625500</td>\n",
       "      <td>2.528045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.649800</td>\n",
       "      <td>2.526106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>2.525519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▆▅█▆▄▁▃▅▂</td></tr><tr><td>eval/samples_per_second</td><td>▄▂▄▁▃▅█▆▄▇</td></tr><tr><td>eval/steps_per_second</td><td>▅▁▅▁▅▅██▅█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.52552</td></tr><tr><td>eval/runtime</td><td>120.0902</td></tr><tr><td>eval/samples_per_second</td><td>25.006</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5625</td></tr><tr><td>train/total_flos</td><td>1.015262467620864e+16</td></tr><tr><td>train/train_loss</td><td>2.64716</td></tr><tr><td>train/train_runtime</td><td>1433.1918</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-sweep-15</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/ccg48k2c' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/ccg48k2c</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_204623-ccg48k2c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rwv8p70z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006078872243379494\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_211047-rwv8p70z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/rwv8p70z' target=\"_blank\">swept-sweep-16</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/rwv8p70z' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/rwv8p70z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,310,720 || all params: 2,786,350,080 || trainable%: 0.04704075088798605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.940900</td>\n",
       "      <td>2.709811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.760900</td>\n",
       "      <td>2.606876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.624700</td>\n",
       "      <td>2.577317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.623100</td>\n",
       "      <td>2.560996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.620700</td>\n",
       "      <td>2.548068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.394000</td>\n",
       "      <td>2.541878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.734600</td>\n",
       "      <td>2.536759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.631100</td>\n",
       "      <td>2.533249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.654700</td>\n",
       "      <td>2.530816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.566900</td>\n",
       "      <td>2.530372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄█▃▅▃▃▅▃▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▁▆▄▇▆▄▆▆█</td></tr><tr><td>eval/steps_per_second</td><td>▅▁█▅██▅███</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.53037</td></tr><tr><td>eval/runtime</td><td>120.0759</td></tr><tr><td>eval/samples_per_second</td><td>25.009</td></tr><tr><td>eval/steps_per_second</td><td>3.131</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5669</td></tr><tr><td>train/total_flos</td><td>1.015513358303232e+16</td></tr><tr><td>train/train_loss</td><td>2.65516</td></tr><tr><td>train/train_runtime</td><td>1432.924</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-sweep-16</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/rwv8p70z' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/rwv8p70z</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_211047-rwv8p70z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a6anmp9q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0013163638178068338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_213520-a6anmp9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/a6anmp9q' target=\"_blank\">clear-sweep-17</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/a6anmp9q' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/a6anmp9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 655,360 || all params: 2,785,694,720 || trainable%: 0.02352590882607553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.950100</td>\n",
       "      <td>2.710745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.756300</td>\n",
       "      <td>2.603949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.625600</td>\n",
       "      <td>2.575514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.620600</td>\n",
       "      <td>2.559472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.619400</td>\n",
       "      <td>2.547351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.396500</td>\n",
       "      <td>2.541637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.734600</td>\n",
       "      <td>2.536533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.629800</td>\n",
       "      <td>2.533366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.652600</td>\n",
       "      <td>2.531205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.565000</td>\n",
       "      <td>2.530154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▄▄▄▆▁▃▂█▂</td></tr><tr><td>eval/samples_per_second</td><td>▆▅▄▅▃█▆▇▁▇</td></tr><tr><td>eval/steps_per_second</td><td>▆▄▄▄▄█▆▆▁▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.53015</td></tr><tr><td>eval/runtime</td><td>120.1124</td></tr><tr><td>eval/samples_per_second</td><td>25.002</td></tr><tr><td>eval/steps_per_second</td><td>3.13</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.565</td></tr><tr><td>train/total_flos</td><td>1.015262467620864e+16</td></tr><tr><td>train/train_loss</td><td>2.65505</td></tr><tr><td>train/train_runtime</td><td>1433.0029</td></tr><tr><td>train/train_samples_per_second</td><td>1.117</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-sweep-17</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/a6anmp9q' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/a6anmp9q</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_213520-a6anmp9q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4m7lh3lk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007312052867119651\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_215943-4m7lh3lk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/4m7lh3lk' target=\"_blank\">blooming-sweep-18</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/4m7lh3lk' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/4m7lh3lk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,621,440 || all params: 2,787,660,800 || trainable%: 0.09403726593995941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 23:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.960600</td>\n",
       "      <td>2.744995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.776400</td>\n",
       "      <td>2.616622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.632700</td>\n",
       "      <td>2.582542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.625400</td>\n",
       "      <td>2.565160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.624100</td>\n",
       "      <td>2.552204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.397200</td>\n",
       "      <td>2.545774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.737900</td>\n",
       "      <td>2.540874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.636700</td>\n",
       "      <td>2.537441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.656700</td>\n",
       "      <td>2.534897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.569800</td>\n",
       "      <td>2.534647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▃▃▂▄▁▅▃▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▇▇▅█▄▆▇█</td></tr><tr><td>eval/steps_per_second</td><td>▁███▄█▄███</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.53465</td></tr><tr><td>eval/runtime</td><td>120.1096</td></tr><tr><td>eval/samples_per_second</td><td>25.002</td></tr><tr><td>eval/steps_per_second</td><td>3.13</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5698</td></tr><tr><td>train/total_flos</td><td>1.016015139667968e+16</td></tr><tr><td>train/train_loss</td><td>2.66175</td></tr><tr><td>train/train_runtime</td><td>1433.5359</td></tr><tr><td>train/train_samples_per_second</td><td>1.116</td></tr><tr><td>train/train_steps_per_second</td><td>0.07</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sweep-18</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/4m7lh3lk' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/4m7lh3lk</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_215943-4m7lh3lk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wsq243kz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005294252418942767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_222428-wsq243kz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/wsq243kz' target=\"_blank\">atomic-sweep-19</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/wsq243kz' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/wsq243kz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,310,720 || all params: 2,786,350,080 || trainable%: 0.04704075088798605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 28:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.993600</td>\n",
       "      <td>2.886605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.866900</td>\n",
       "      <td>2.660721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.676500</td>\n",
       "      <td>2.617707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>2.593578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.651200</td>\n",
       "      <td>2.576303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.417500</td>\n",
       "      <td>2.568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.760100</td>\n",
       "      <td>2.562615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.653700</td>\n",
       "      <td>2.559029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.675700</td>\n",
       "      <td>2.555943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.591500</td>\n",
       "      <td>2.555135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁▃██████</td></tr><tr><td>eval/samples_per_second</td><td>███▆▁▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>███▆▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.55514</td></tr><tr><td>eval/runtime</td><td>157.0404</td></tr><tr><td>eval/samples_per_second</td><td>19.122</td></tr><tr><td>eval/steps_per_second</td><td>2.394</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5915</td></tr><tr><td>train/total_flos</td><td>1.015513358303232e+16</td></tr><tr><td>train/train_loss</td><td>2.69367</td></tr><tr><td>train/train_runtime</td><td>1696.4615</td></tr><tr><td>train/train_samples_per_second</td><td>0.943</td></tr><tr><td>train/train_steps_per_second</td><td>0.059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-19</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/wsq243kz' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/wsq243kz</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_222428-wsq243kz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vqem7ulk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0013222286150400846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/jp-finetuning/wandb/run-20231007_225330-vqem7ulk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/vqem7ulk' target=\"_blank\">revived-sweep-20</a></strong> to <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/sweeps/8e17pj90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/vqem7ulk' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/vqem7ulk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,621,440 || all params: 2,787,660,800 || trainable%: 0.09403726593995941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 30:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.926100</td>\n",
       "      <td>2.679382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.737500</td>\n",
       "      <td>2.592502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.613700</td>\n",
       "      <td>2.573694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.617700</td>\n",
       "      <td>2.551992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.611000</td>\n",
       "      <td>2.540586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.386500</td>\n",
       "      <td>2.533543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.728900</td>\n",
       "      <td>2.529042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.622800</td>\n",
       "      <td>2.525728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.648300</td>\n",
       "      <td>2.523227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.560900</td>\n",
       "      <td>2.522573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-25)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-50)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-75)... Done. 0.0s\n",
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-100)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>██▆█▇▆▅▆▁█</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▃▁▂▃▄▃█▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▃▂▂▄▄▃█▂</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▁▅▄▄▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.52257</td></tr><tr><td>eval/runtime</td><td>157.2163</td></tr><tr><td>eval/samples_per_second</td><td>19.101</td></tr><tr><td>eval/steps_per_second</td><td>2.392</td></tr><tr><td>train/epoch</td><td>0.13</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5609</td></tr><tr><td>train/total_flos</td><td>1.016015139667968e+16</td></tr><tr><td>train/train_loss</td><td>2.64533</td></tr><tr><td>train/train_runtime</td><td>1855.3999</td></tr><tr><td>train/train_samples_per_second</td><td>0.862</td></tr><tr><td>train/train_steps_per_second</td><td>0.054</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-20</strong> at: <a href='https://wandb.ai/japan-demo/jp-instruction-tuning/runs/vqem7ulk' target=\"_blank\">https://wandb.ai/japan-demo/jp-instruction-tuning/runs/vqem7ulk</a><br/>Synced 6 W&B file(s), 0 media file(s), 39 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231007_225330-vqem7ulk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_configuration= {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"eval/loss\"},\n",
    "    \"parameters\": {\n",
    "        \"r\":{\"values\": [2,4,8,16,32]},\n",
    "        \"lora_alpha\":{\"values\": [2,4,8,16]},\n",
    "        \"learning_rate\":{'max': 2e-3, 'min': 2e-4}\n",
    "    }\n",
    "}\n",
    "\n",
    "default_config = {\n",
    "    \"BASE_MODEL\":\"cyberagent/open-calm-3b\",\n",
    "    \"lora_config\":{\n",
    "        \"r\":32,\n",
    "        \"lora_alpha\":16,\n",
    "        \"target_modules\":[\"query_key_value\"],\n",
    "        \"lora_dropout\":.1,\n",
    "        \"bias\":\"none\",\n",
    "        \"task_type\":\"CAUSAL_LM\"\n",
    "    },\n",
    "    \"training_args\":{\n",
    "        \"dataloader_num_workers\":16,\n",
    "        \"evaluation_strategy\":\"steps\",\n",
    "        \"per_device_train_batch_size\":8,\n",
    "        \"max_steps\": 100,\n",
    "        \"gradient_accumulation_steps\":2,\n",
    "        \"report_to\":\"wandb\",#wandb integration\n",
    "        \"warmup_steps\":10,\n",
    "        \"num_train_epochs\":1,\n",
    "        \"learning_rate\":2e-4,\n",
    "        \"fp16\":True,\n",
    "        \"logging_steps\":10,\n",
    "        \"save_steps\":25,\n",
    "        \"output_dir\":'./outputs'\n",
    "    }\n",
    "}\n",
    "\n",
    "# cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float32)\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.embed_out = CastOutputToFloat(model.embed_out)\n",
    "\n",
    "\n",
    "def train_func():\n",
    "    with wandb.init(config=config, job_type=\"training\") as run:\n",
    "        # Setup for LoRa\n",
    "        default_config[\"lora_config\"][\"r\"] = wandb.config[\"r\"]\n",
    "        default_config[\"lora_config\"][\"lora_alpha\"] = wandb.config[\"lora_alpha\"]\n",
    "        default_config[\"training_args\"][\"learning_rate\"] = wandb.config[\"learning_rate\"]\n",
    "\n",
    "\n",
    "        lora_config = LoraConfig(**default_config[\"lora_config\"])\n",
    "        model_peft = get_peft_model(model, lora_config)    \n",
    "        model_peft.print_trainable_parameters()\n",
    "        model_peft.config.use_cache = False\n",
    "    \n",
    "        trainer = transformers.Trainer(\n",
    "            model=model_peft,\n",
    "            data_collator=collator,\n",
    "            args=transformers.TrainingArguments(**default_config[\"training_args\"]),\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset\n",
    "        )\n",
    "        trainer.train()\n",
    "        model_peft.save_pretrained(\"./output\")\n",
    "        model_ft = wandb.Artifact(f\"finetuned-model\", type=\"model\")\n",
    "        model_ft.add_dir(\"./output\")\n",
    "        run.log_artifact(model_ft)\n",
    "        run.log_code()\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration)\n",
    "wandb.agent(sweep_id, function=train_func, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
