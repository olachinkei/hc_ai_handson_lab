{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM JP instruction-tuning Finetuning with HuggingFace and Weights and Biases\n",
    "<!--- @wandbcode{llm-finetune-hf} -->\n",
    "- Fine-tune a lightweight LLM with LoRA and 8-bit quantization\n",
    "- Checkpoint the LoRA adapter weights as artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install update -q bitsandbytes datasets accelerate loralib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import wandb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeisuke-kamata\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"japan-demo\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"jp-instruction-tuning\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=1024\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    BASE_MODEL=\"cyberagent/open-calm-7b\",\n",
    "    lora_config=SimpleNamespace(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"query_key_value\"],\n",
    "        lora_dropout=.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    ),\n",
    "    training=SimpleNamespace(\n",
    "        dataloader_num_workers=16,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        report_to=\"wandb\",#wandb integration\n",
    "        warmup_steps=10,\n",
    "        max_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=25,\n",
    "        output_dir='./outputs'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mBASE_MODEL)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:2401\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2402\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2403\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2404\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pip install bitsandbytes` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2405\u001b[0m         )\n\u001b[1;32m   2407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2408\u001b[0m         \u001b[38;5;66;03m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2410\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverriding torch_dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with `torch_dtype=torch.float16` due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2413\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch_dtype=torch.float16 to remove this warning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2414\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_ja = datasets.load_dataset(\"kunishou/databricks-dolly-15k-ja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_INPUT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response\"\"\"\n",
    "\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "Input:\n",
    "{context}\n",
    "### Response\"\"\"\n",
    "\n",
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, json_list, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = ignore_index\n",
    "        self.features = []\n",
    "        \n",
    "        for j in tqdm(json_list):\n",
    "            # In cases like open_qa where context information is not necessary, there is no input column.\n",
    "            # Therefore, we differentiate the template sentences based on whether the input column is present or not.\n",
    "            if 'input' in j:\n",
    "                source_text = PROMPT_DICT['prompt_input'].format_map(j)\n",
    "            else:\n",
    "                source_text = PROMPT_DICT['prompt_no_input'].format_map(j)\n",
    "            # Combine the instruction sentence and the response sentence, and insert an EOS token at the end\n",
    "            example_text = source_text + j['output'] + self.tokenizer.eos_token\n",
    "            # okenize only the instruction sentence (up to 'The following is a task to ~### Response:')\n",
    "            # What we want is the length of the instruction sentence.\n",
    "            source_tokenized = self.tokenizer(\n",
    "                source_text,\n",
    "                padding='longest',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_length=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Tokenize both the instruction sentence and the response sentence\n",
    "            example_tokenized = self.tokenizer(\n",
    "                example_text, \n",
    "                padding='longest', \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = example_tokenized['input_ids'][0]\n",
    "            \n",
    "            # Copy the input sentence as is to be the correct answer that the LLM generates.\n",
    "            labels = copy.deepcopy(input_ids)\n",
    "            \n",
    "            # Length up to the instruction sentence\n",
    "            source_len = source_tokenized['length'][0]\n",
    "            \n",
    "            # Since the desired correct sentence for the LLM to generate also includes the instruction sentence,\n",
    "            # we fill the section of the instruction sentence with -100 as IGNORE_INDEX to avoid calculating the CrossEntropyLoss.\n",
    "            labels[:source_len] = self.ignore_index\n",
    "            \n",
    "            self.features.append({\n",
    "                'input_ids': input_ids,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "class InstructCollator():\n",
    "    def __init__(self, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = -100\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        input_batch = []\n",
    "        label_batch = []\n",
    "        for example in examples:\n",
    "            input_batch.append(example['input_ids'])\n",
    "            label_batch.append(example['labels'])\n",
    "        input_ids = pad_sequence(\n",
    "            input_batch, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = pad_sequence(\n",
    "            label_batch, batch_first=True, padding_value=self.ignore_index\n",
    "        )\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)   \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InstructDataset(dolly_ja, tokenizer)\n",
    "collator = InstructCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの中身を確認\n",
    "print(model.gpt_neox.layers[0].attention)\n",
    "#GPTNeoXAttention(\n",
    "#  (rotary_emb): RotaryEmbedding()\n",
    "#  (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
    "#  (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "\n",
    "# cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float32)\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(config=config, job_type=\"training\") as run:\n",
    "    # Setup for LoRa\n",
    "    config = wandb.config\n",
    "    \n",
    "    lora_config = LoraConfig(**config[\"lora_config\"])\n",
    "    model_peft = get_peft_model(model, lora_config)    \n",
    "    model_peft.print_trainable_parameters()\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model_peft,\n",
    "        data_collator=collator,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    model_peft.save_pretrained(\"./output\")\n",
    "    model_ft = wandb.Artifact(f\"finetuned-model\", type=\"model\")\n",
    "    model_ft.add_dir(\"./output\")\n",
    "    run.log_artifact(model_ft)\n",
    "    run.log_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Advanced) Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    BASE_MODEL=\"cyberagent/open-calm-7b\",\n",
    "    lora_config=SimpleNamespace(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"query_key_value\"],\n",
    "        lora_dropout=.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    ),\n",
    "    training=SimpleNamespace(\n",
    "        dataloader_num_workers=16,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        report_to=\"wandb\",\n",
    "        warmup_steps=10,\n",
    "        max_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=25,\n",
    "        output_dir='./outputs',\n",
    "        report_to=\"wandb\", #wandb integration\n",
    "    )\n",
    ")\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"loss\"},\n",
    "    \"parameters\": {\n",
    "        \"lora_config.r\": {\"values\": [2,4,8,16,32]}\n",
    "        \"lora_config.lora_alpha\": {\"values\": [2,4,8,16]},\n",
    "        \"training.learning_rate\": {'max': 2e-3, 'min': 2e-4},\n",
    "    },\n",
    "}\n",
    "\n",
    "for param in model.parameters():\n",
    "        param.requires_grad = False # freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            param.data = param.data.to(torch.float32)\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "def train_func():\n",
    "    with wandb.init(config=config, job_type=\"training\") as run:\n",
    "    # Setup for LoRa\n",
    "    config = wandb.config    \n",
    "    lora_config = LoraConfig(**config[\"lora_config\"])\n",
    "    model_peft = get_peft_model(model, lora_config)\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model_peft,\n",
    "        data_collator=collator,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    model_peft.save_pretrained(\"./output\")\n",
    "    model_ft = wandb.Artifact(f\"finetuned-model\", type=\"model\")\n",
    "    model_ft.add_dir(\"./output\")\n",
    "    run.log_artifact(model_ft)\n",
    "    run.log_code()\n",
    "    \n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration)\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id, function=my_train_func)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
