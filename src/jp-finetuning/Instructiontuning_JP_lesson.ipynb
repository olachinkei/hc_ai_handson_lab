{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM JP instruction-tuning Finetuning with HuggingFace and Weights and Biases\n",
    "<!--- @wandbcode{llm-finetune-hf} -->\n",
    "- Fine-tune a lightweight LLM with LoRA and 8-bit quantization\n",
    "- Checkpoint the LoRA adapter weights as artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import wandb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"japan-demo\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"jp-instruction-tuning\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=1024\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd53f193cce74c638892e2ee4e93c666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7618dbf5fa7e490fa28bbab15f4c88a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad1458645ce418aaf2c9399f0eb6cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = SimpleNamespace(\n",
    "    BASE_MODEL=\"cyberagent/open-calm-7b\",\n",
    "    lora_config=SimpleNamespace(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"query_key_value\"],\n",
    "        lora_dropout=.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    ),\n",
    "    training=SimpleNamespace(\n",
    "        dataloader_num_workers=16,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        report_to=\"wandb\",\n",
    "        warmup_steps=10,\n",
    "        max_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=25,\n",
    "        output_dir='./outputs',\n",
    "        report_to=\"wandb\", #wandb integration\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_ja = datasets.load_dataset(\"kunishou/databricks-dolly-15k-ja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_INPUT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response\"\"\"\n",
    "\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "Input:\n",
    "{context}\n",
    "### Response\"\"\"\n",
    "\n",
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, json_list, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = ignore_index\n",
    "        self.features = []\n",
    "        \n",
    "        for j in tqdm(json_list):\n",
    "            # open_qaなど文脈情報が必要ない場合はinputカラムがないため、\n",
    "            # inputカラムありなしでテンプレート文を分けている。\n",
    "            if 'input' in j:\n",
    "                source_text = PROMPT_DICT['prompt_input'].format_map(j)\n",
    "            else:\n",
    "                source_text = PROMPT_DICT['prompt_no_input'].format_map(j)\n",
    "            # 指示文と回答文を結合し、文末にEOSトークンを挿入\n",
    "            example_text = source_text + j['output'] + self.tokenizer.eos_token\n",
    "            # 指示文のみ（「以下は、タスクを〜### 応答:」まで）をtokenize\n",
    "            # ほしいのは指示文のlength\n",
    "            source_tokenized = self.tokenizer(\n",
    "                source_text,\n",
    "                padding='longest',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_length=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # 指示文と回答文を全てtokenize\n",
    "            example_tokenized = self.tokenizer(\n",
    "                example_text, \n",
    "                padding='longest', \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = example_tokenized['input_ids'][0]\n",
    "            \n",
    "            # LLMが生成してほしい正解の文章として入力文をそのままコピーする\n",
    "            labels = copy.deepcopy(input_ids)\n",
    "            \n",
    "            # 指示文までの長さ\n",
    "            source_len = source_tokenized['length'][0]\n",
    "            \n",
    "            # LLMに生成してほしい正解文章に指示文も含まれているので、\n",
    "            # 指示文のところはCrossEntropyLossの損失を計算をしないようにIGNORE_INDEXとして-100で埋める\n",
    "            labels[:source_len] = self.ignore_index\n",
    "            \n",
    "            self.features.append({\n",
    "                'input_ids': input_ids,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "class InstructCollator():\n",
    "    def __init__(self, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = -100\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        input_batch = []\n",
    "        label_batch = []\n",
    "        for example in examples:\n",
    "            input_batch.append(example['input_ids'])\n",
    "            label_batch.append(example['labels'])\n",
    "        input_ids = pad_sequence(\n",
    "            input_batch, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        # labelsのpaddingトークンは先程と同様にignore_indexである-100で埋める\n",
    "        labels = pad_sequence(\n",
    "            label_batch, batch_first=True, padding_value=self.ignore_index\n",
    "        )\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)   \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InstructDataset(dolly_ja, tokenizer)\n",
    "collator = InstructCollator(tokenizer)\n",
    "\n",
    "# 中身の確認\n",
    "#loader = DataLoader(train_dataset, collate_fn=collator, batch_size=8, shuffle=True)\n",
    "#batch = next(iter(loader))\n",
    "#batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(config=config, job_type=\"training\") as run:\n",
    "    # Setup for LoRa\n",
    "    config = wandb.config\n",
    "    #モデル構築のための準備\n",
    "    # モデルの中身を確認\n",
    "    print(model.gpt_neox.layers[0].attention)\n",
    "    #GPTNeoXAttention(\n",
    "    #  (rotary_emb): RotaryEmbedding()\n",
    "    #  (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
    "    #  (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    #)\n",
    "    \n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False # freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            param.data = param.data.to(torch.float32)\n",
    "    model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "    model.enable_input_require_grads()\n",
    "    class CastOutputToFloat(nn.Sequential):\n",
    "        def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "     \n",
    "    lora_config = LoraConfig(**config[\"lora_config\"])\n",
    "    model_peft = get_peft_model(model, lora_config)\n",
    "    #いくつのパラメータで学習をするかを確認\n",
    "    model_peft.print_trainable_parameters()\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model_peft,\n",
    "        data_collator=collator,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    \n",
    "    # モデル構築\n",
    "    trainer.train()\n",
    "    \n",
    "    # モデルの保存\n",
    "    model_peft.save_pretrained(\"./output\")\n",
    "    model_ft = wandb.Artifact(f\"finetuned-model\", type=\"model\")\n",
    "    model_ft.add_dir(\"./output\")\n",
    "    run.log_artifact(model_ft)\n",
    "    run.log_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Advanced) Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    BASE_MODEL=\"cyberagent/open-calm-7b\",\n",
    "    lora_config=SimpleNamespace(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"query_key_value\"],\n",
    "        lora_dropout=.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    ),\n",
    "    training=SimpleNamespace(\n",
    "        dataloader_num_workers=16,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        report_to=\"wandb\",\n",
    "        warmup_steps=10,\n",
    "        max_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=25,\n",
    "        output_dir='./outputs',\n",
    "        report_to=\"wandb\", #wandb integration\n",
    "    )\n",
    ")\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"loss\"},\n",
    "    \"parameters\": {\n",
    "        \"lora_config.r\": {\"values\": [2,4,8,16,32]}\n",
    "        \"lora_config.lora_alpha\": {\"values\": [2,4,8,16]},\n",
    "        \"training.learning_rate\": {'max': 2e-3, 'min': 2e-4},\n",
    "    },\n",
    "}\n",
    "\n",
    "for param in model.parameters():\n",
    "        param.requires_grad = False # freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            param.data = param.data.to(torch.float32)\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "def train_func():\n",
    "    with wandb.init(config=config, job_type=\"training\") as run:\n",
    "    # Setup for LoRa\n",
    "    config = wandb.config    \n",
    "    lora_config = LoraConfig(**config[\"lora_config\"])\n",
    "    model_peft = get_peft_model(model, lora_config)\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model_peft,\n",
    "        data_collator=collator,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    model_peft.save_pretrained(\"./output\")\n",
    "    model_ft = wandb.Artifact(f\"finetuned-model\", type=\"model\")\n",
    "    model_ft.add_dir(\"./output\")\n",
    "    run.log_artifact(model_ft)\n",
    "    run.log_code()\n",
    "    \n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration)\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id, function=my_train_func)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
