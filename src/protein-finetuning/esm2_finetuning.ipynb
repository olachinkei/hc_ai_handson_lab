{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9a00f8-0465-4c86-853a-2af8487a73df",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c48a81e-c461-4e7d-9826-4c1b3574b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeisuke-kamata\u001b[0m (\u001b[33mhc-ai-handson\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, matthews_corrcoef, accuracy_score\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from accelerate import Accelerator\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "# Setup Environment Variables and Accelerator\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Setup Wandb\n",
    "os.environ[\"WANDB_ENTITY\"] = \"hc-ai-handson\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "project=\"esm2-binding-sites\"\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dbec0-6640-4282-90d5-a3876f4acf82",
   "metadata": {},
   "source": [
    "# Data prepartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01c26a-bd41-4d95-b505-4165f1a6e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_url(url,name):\n",
    "    response = requests.get(url)\n",
    "    with open(name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "with wandb.init(project=project,job_type=\"upload_data\") as run:\n",
    "    artifact = wandb.Artifact(name=\"binding_sites_random_split_by_family_train\",\n",
    "                              metadata={\n",
    "                                        \"url\": \"https://huggingface.co/datasets/AmelieSchreiber/binding_sites_random_split_by_family_550K\",\n",
    "                                        },\n",
    "                              type=\"dataset\")\n",
    "    url=\"https://cdn-lfs.huggingface.co/repos/f1/20/f1203a07ea684a9586a90e512fe0ab40290bbaa0f57833aed3e29decf5520637/f17f5ca4beb72ba0a867c94da2f145a1cb7924f8013461dcf39384555ccc3d79?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train_labels_chunked_by_family.pkl%3B+filename%3D%22train_labels_chunked_by_family.pkl%22%3B&Expires=1695257172&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NTI1NzE3Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mMS8yMC9mMTIwM2EwN2VhNjg0YTk1ODZhOTBlNTEyZmUwYWI0MDI5MGJiYWEwZjU3ODMzYWVkM2UyOWRlY2Y1NTIwNjM3L2YxN2Y1Y2E0YmViNzJiYTBhODY3Yzk0ZGEyZjE0NWExY2I3OTI0ZjgwMTM0NjFkY2YzOTM4NDU1NWNjYzNkNzk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jtL4YhzDy5LYzD3IvmWmPG0u4XlGQB6a48KPkKGFtyirCUvCeywmlffV6RRqwwX1ROChlcDM-ytSPUJ5%7Ef3EkhRPogFCyTnKGJJlZ9DNYyF8qCb-s4FdjPZeh2SPXzGDvoDgtBOfgOv-yx%7Evyk8GILBYfgjZqCKj1I7LByOUmslSYIYNsZOp90bLtVlbQOO3V7EpGnc4iRawkcA90D1I0zwUFfvOxZ6K7mZW4en3nXlNOTZ5e71uI5Kmar%7EY4f6wMYn5B5HOiSL%7EwEww-AUlicMcvkawmVBxTxKoNjZ6M%7EQJ0fnyyzKyXW6rrpjojfEprUs1D9JtCaZlKxrWTnIeSQ__&Key-Pair-Id=KVTP0A1DKRTAX\"\n",
    "    get_data_from_url(url,\"train_labels_chunked_by_family.pkl\")\n",
    "    url=\"https://cdn-lfs.huggingface.co/repos/f1/20/f1203a07ea684a9586a90e512fe0ab40290bbaa0f57833aed3e29decf5520637/c5fb314b71338ce943c62301e00f1fd865afa50bf2d7a6440a2935db30989e45?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train_sequences_chunked_by_family.pkl%3B+filename%3D%22train_sequences_chunked_by_family.pkl%22%3B&Expires=1695257198&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NTI1NzE5OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mMS8yMC9mMTIwM2EwN2VhNjg0YTk1ODZhOTBlNTEyZmUwYWI0MDI5MGJiYWEwZjU3ODMzYWVkM2UyOWRlY2Y1NTIwNjM3L2M1ZmIzMTRiNzEzMzhjZTk0M2M2MjMwMWUwMGYxZmQ4NjVhZmE1MGJmMmQ3YTY0NDBhMjkzNWRiMzA5ODllNDU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Dh2R5oGLfwPWWce97ZJsihX-mSTFfwbtEfq1SI00tVyacaT4dlxnCddshe%7E15tSamiMukONmeTP38hQUmRitP1-v3wjZ3%7E5G7OeBmsWeK5%7Eivva7xcJOOe45KUXgy59fIkBneULZOLw8ysrkD83IGkybkRB3MlNWLeiwHIai9cPgqdF-yCk1nxU2VHL-A2GUh2W3sTo7Q1edE3Qzy2NV2QSbfmadsz1xIYBh135d%7EzJp0Y7MnNGZV-UyGBJJXjgOjwU7B05esYekTjuQTaX9ujh3iEF883SBepgtWypJ81%7EZ7LYYpuvGwCv5eO2-6qf9kMtLZQLww87US4C1%7EWOxMQ__&Key-Pair-Id=KVTP0A1DKRTAX\"\n",
    "    get_data_from_url(url,\"train_sequences_chunked_by_family.pkl\")\n",
    "    artifact.add_file(local_path=\"train_sequences_chunked_by_family.pkl\")\n",
    "    artifact.add_file(local_path=\"train_labels_chunked_by_family.pkl\")\n",
    "    run.log_artifact(artifact)\n",
    "    \n",
    "with wandb.init(project=project,job_type=\"upload_data\") as run:\n",
    "    artifact = wandb.Artifact(name=\"binding_sites_random_split_by_family_test\",\n",
    "                              metadata={\n",
    "                                        \"url\": \"https://huggingface.co/datasets/AmelieSchreiber/binding_sites_random_split_by_family_550K\",\n",
    "                                        },\n",
    "                              type=\"dataset\")\n",
    "    url=\"https://cdn-lfs.huggingface.co/repos/f1/20/f1203a07ea684a9586a90e512fe0ab40290bbaa0f57833aed3e29decf5520637/af3ddcda3b27eeac15739f06046b122ae5bfac944929434c53fbd2a91e44684c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27test_labels_chunked_by_family.pkl%3B+filename%3D%22test_labels_chunked_by_family.pkl%22%3B&Expires=1695256911&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NTI1NjkxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mMS8yMC9mMTIwM2EwN2VhNjg0YTk1ODZhOTBlNTEyZmUwYWI0MDI5MGJiYWEwZjU3ODMzYWVkM2UyOWRlY2Y1NTIwNjM3L2FmM2RkY2RhM2IyN2VlYWMxNTczOWYwNjA0NmIxMjJhZTViZmFjOTQ0OTI5NDM0YzUzZmJkMmE5MWU0NDY4NGM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=0Z%7ENnciKC5NA3W47qQxn6t4KGdDY65%7Efma0Kq7Oyfd79xnxZVerXHSD8jeWK52iPP-My7DbqpzWdvQNtLupHzzPhzqqQ%7EUwZfr%7EwbnHOvUCFLGQJczkCWXyQO90cDyzXm5vNR1lG%7EcM5V1COhWeOKRkToeqzeTvaL4e1bTN71ZLCepfYRzGr8le5xHNPTNmhnnpIldkuB4guUGYsJFirdJ5bJT3ZwLRq0hN3EMvPimoORfyVcO%7E-pYN8%7ENqnaQroPjrc8SnVA2BXmZWyyozYZ2vfWw83rnVB%7EYWJcRO46uBqK0tW5bgeBWcI98qyBO458vjyzpdG5skoz7Cq3OwiZA__&Key-Pair-Id=KVTP0A1DKRTAX\"\n",
    "    get_data_from_url(url,\"test_labels_chunked_by_family.pkl\")\n",
    "    url=\"https://cdn-lfs.huggingface.co/repos/f1/20/f1203a07ea684a9586a90e512fe0ab40290bbaa0f57833aed3e29decf5520637/e304531c74a93d537f17b00585424c87afd6b130915dde22f356892b4cb3b240?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27test_sequences_chunked_by_family.pkl%3B+filename%3D%22test_sequences_chunked_by_family.pkl%22%3B&Expires=1695257137&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NTI1NzEzN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mMS8yMC9mMTIwM2EwN2VhNjg0YTk1ODZhOTBlNTEyZmUwYWI0MDI5MGJiYWEwZjU3ODMzYWVkM2UyOWRlY2Y1NTIwNjM3L2UzMDQ1MzFjNzRhOTNkNTM3ZjE3YjAwNTg1NDI0Yzg3YWZkNmIxMzA5MTVkZGUyMmYzNTY4OTJiNGNiM2IyNDA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ATB3zaFSwDQ9TClQPD0ERVzTjjeEJZamzZjjnXeda3ecjVCJoKCycIQ-o0zoJL1b0F9NShZyUVmCPQqbMA4fn7RhA63lbOv8%7EBrjD3FnqUVnXaOADvmGhCS0Uq2aFpEM9Z0aLn8rpPt04gIU3MpDGjqRVUX8TlANZcp8y4fXPj9IcEXKT7xIlkhnDoG5Pj4gJzNMziHYRfW4q792pKfjl0XEBeBHEuHxXSBG5CsalXGN1OAb3w2JOruLI0v4L461TPHSPAAuZTYI-KeMhZV1zAMEWQ4ODEKWA%7EcPa94HlKnsC7DEVY-kcPUET98kRbtHpTDH7s9OA51kPJoYk-sZVg__&Key-Pair-Id=KVTP0A1DKRTAX\"\n",
    "    get_data_from_url(url,\"test_sequences_chunked_by_family.pkl\")\n",
    "    artifact.add_file(local_path=\"test_sequences_chunked_by_family.pkl\")\n",
    "    artifact.add_file(local_path=\"test_labels_chunked_by_family.pkl\")\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca30e4-c2c6-4074-9e97-87eafd872989",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb55858-bb20-4695-b9df-ee97e8d5c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binding_string_to_labels(binding_string):\n",
    "    \"\"\"Convert 'proBnd' strings into label arrays.\"\"\"\n",
    "    return [1 if char == '+' else 0 for char in binding_string]\n",
    "\n",
    "def truncate_labels(labels, max_length):\n",
    "    \"\"\"Truncate labels to the specified max_length.\"\"\"\n",
    "    return [label[:max_length] for label in labels]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions[labels != -100].flatten()\n",
    "    labels = labels[labels != -100].flatten()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n",
    "\n",
    "def compute_loss(model, inputs):\n",
    "    \"\"\"Custom compute_loss function.\"\"\"\n",
    "    logits = model(**inputs).logits\n",
    "    labels = inputs[\"labels\"]\n",
    "    loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    active_loss = inputs[\"attention_mask\"].view(-1) == 1\n",
    "    active_logits = logits.view(-1, model.config.num_labels)\n",
    "    \n",
    "    # The torch.where function is used to obtain the labels at the positions of the active tokens,\n",
    "    # and set the ignore index at the positions of inactive tokens (padding tokens).\n",
    "    active_labels = torch.where(\n",
    "        active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "    )\n",
    "    loss = loss_fct(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64998098-e38d-45ef-b1a9-0dda3b1ed5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(config,download=False):\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n",
    "    max_sequence_length = 1000\n",
    "    \n",
    "    # Use artifacts on wandb\n",
    "    if download:\n",
    "        with wandb.init(project=config[\"wandb_project\"], job_type=\"data_prep\") as run:\n",
    "            artifact_train = run.use_artifact('hc-ai-handson/esm2-binding-sites/binding_sites_random_split_by_family_train:v0', type='dataset')\n",
    "            artifact_test = run.use_artifact('hc-ai-handson/esm2-binding-sites/binding_sites_random_split_by_family_test:v0', type='dataset')\n",
    "            artifact_dir_train = artifact_train.download()\n",
    "            artifact_dir_test = artifact_test.download()\n",
    "            # Load the data from pickle files\n",
    "            with open(artifact_dir_train+\"/train_sequences_chunked_by_family.pkl\", \"rb\") as f:\n",
    "                train_sequences = pickle.load(f)\n",
    "            with open(artifact_dir_train+\"/train_labels_chunked_by_family.pkl\", \"rb\") as f:\n",
    "                train_labels = pickle.load(f)\n",
    "            with open(artifact_dir_test+\"/test_sequences_chunked_by_family.pkl\", \"rb\") as f:\n",
    "                test_sequences = pickle.load(f)\n",
    "            with open(artifact_dir_test+\"/test_labels_chunked_by_family.pkl\", \"rb\") as f:\n",
    "                test_labels = pickle.load(f)\n",
    "    else:\n",
    "        with open(\"train_sequences_chunked_by_family.pkl\", \"rb\") as f:\n",
    "            train_sequences = pickle.load(f)\n",
    "        with open(\"train_labels_chunked_by_family.pkl\", \"rb\") as f:\n",
    "            train_labels = pickle.load(f)\n",
    "        with open(\"test_sequences_chunked_by_family.pkl\", \"rb\") as f:\n",
    "            test_sequences = pickle.load(f)\n",
    "        with open(\"test_labels_chunked_by_family.pkl\", \"rb\") as f:\n",
    "            test_labels = pickle.load(f)\n",
    "        \n",
    "    train_tokenized = tokenizer(train_sequences, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)\n",
    "    test_tokenized = tokenizer(test_sequences, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)\n",
    "    \n",
    "    # Directly truncate the entire list of labels\n",
    "    train_labels = truncate_labels(train_labels, max_sequence_length)\n",
    "    test_labels = truncate_labels(test_labels, max_sequence_length)\n",
    "    \n",
    "    train_dataset = Dataset.from_dict({k: v for k, v in train_tokenized.items()}).add_column(\"labels\", train_labels)\n",
    "    test_dataset = Dataset.from_dict({k: v for k, v in test_tokenized.items()}).add_column(\"labels\", test_labels)\n",
    "    \n",
    "    # Compute Class Weights\n",
    "    classes = [0, 1]  \n",
    "    flat_train_labels = [label for sublist in train_labels for label in sublist]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=flat_train_labels)\n",
    "    accelerator = Accelerator()\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(accelerator.device)\n",
    "            \n",
    "    return train_dataset, test_dataset, class_weights, tokenizer\n",
    "    \n",
    "# Define Custom Trainer Class\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        loss = compute_loss(model, inputs)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4f4cc3-48af-4358-a70b-afc22cd6dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning(train_dataset,test_dataset,config):\n",
    "    # Define labels and model\n",
    "    id2label = {0: \"No binding site\", 1: \"Binding site\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Train and Save Model\n",
    "    with wandb.init(project=config[\"wandb_project\"],config=config,tags=[\"finetuning\"]) as run:\n",
    "        config=wandb.config\n",
    "        base_model = AutoModelForTokenClassification.from_pretrained(config.base_model_path, \n",
    "                                                                     num_labels=len(id2label),\n",
    "                                                                     id2label=id2label,\n",
    "                                                                     label2id=label2id)\n",
    "        # Convert the model into a PeftModel\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.TOKEN_CLS, \n",
    "            inference_mode=False, \n",
    "            r=config[\"r\"], \n",
    "            lora_alpha=config.lora_alpha, \n",
    "            target_modules=[\"query\", \"key\", \"value\"], # also try \"dense_h_to_4h\" and \"dense_4h_to_h\"\n",
    "            lora_dropout=config.lora_dropout, \n",
    "            bias=\"none\" # or \"all\" or \"lora_only\" \n",
    "        )\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "      \n",
    "        # Use the accelerator\n",
    "        accelerator = Accelerator()\n",
    "        model = accelerator.prepare(model)\n",
    "        train_dataset = accelerator.prepare(train_dataset)\n",
    "        test_dataset = accelerator.prepare(test_dataset)\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    \n",
    "        # Training setup\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"checkpoint/finetuned_model_{timestamp}\",\n",
    "            learning_rate=config.lr,\n",
    "            lr_scheduler_type=config.lr_scheduler_type,\n",
    "            gradient_accumulation_steps=1,\n",
    "            max_grad_norm=config.max_grad_norm,\n",
    "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=config.per_device_train_batch_size,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            push_to_hub=False,\n",
    "            logging_dir=None,\n",
    "            logging_first_step=False,\n",
    "            logging_steps=200,\n",
    "            save_total_limit=7,\n",
    "            no_cuda=False,\n",
    "            seed=8893,\n",
    "            fp16=True,\n",
    "            report_to='wandb',\n",
    "        )\n",
    "    \n",
    "        # Initialize Trainer\n",
    "        trainer = WeightedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        lora_model_path = os.path.join(\"finetuned_model\", f\"best_model_esm2_lora_{timestamp}\")\n",
    "        trainer.save_model(lora_model_path)\n",
    "        tokenizer.save_pretrained(lora_model_path)\n",
    "        \n",
    "        # Load the LoRA model\n",
    "        #finetuned_model = PeftModel.from_pretrained(base_model, lora_model_path, torch_dtype=torch.float16)\n",
    "        #finetuned_model = accelerator.prepare(finetuned_model)\n",
    "\n",
    "        # Create a data collator\n",
    "        #data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "        \n",
    "        # Get the metrics for the training and test datasets\n",
    "        #train_metrics = compute_metrics_evalaution(train_dataset, finetuned_model, data_collator,\"train_\")\n",
    "        #test_metrics = compute_metrics_evalaution(test_dataset, finetuned_model, data_collator,\"test_\")\n",
    "        #run.log(train_metrics)\n",
    "        #run.log(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c839f6-baee-473d-a376-d709da71427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"base_model_path\": \"facebook/esm2_t6_8M_UR50D\",\n",
    "    \"lora_alpha\": 1,\n",
    "    \"lora_dropout\": 0.2,\n",
    "    \"lr\": 5e-03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"num_train_epochs\": 4,\n",
    "    \"per_device_train_batch_size\": 12,\n",
    "    \"r\": 2,\n",
    "    \"weight_decay\": 0.2,\n",
    "    \"wandb_project\": project,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8503e0d5-d32c-4987-a15b-95b81486e6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, class_weights, tokenizer = data_preparation(config,download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aeff331-0a73-4eca-a2a6-c3630a9dfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = train_dataset.select(range(100))\n",
    "#test_dataset = test_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f2320-1398-4fd3-944c-16a1bb327ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/wandb/run-20230918_083017-gksls3fv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hc-ai-handson/esm2-binding-sites/runs/gksls3fv' target=\"_blank\">floral-blaze-13</a></strong> to <a href='https://wandb.ai/hc-ai-handson/esm2-binding-sites' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hc-ai-handson/esm2-binding-sites' target=\"_blank\">https://wandb.ai/hc-ai-handson/esm2-binding-sites</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hc-ai-handson/esm2-binding-sites/runs/gksls3fv' target=\"_blank\">https://wandb.ai/hc-ai-handson/esm2-binding-sites/runs/gksls3fv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150113' max='150112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150112/150112 8:22:54, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.408267</td>\n",
       "      <td>0.246493</td>\n",
       "      <td>0.764770</td>\n",
       "      <td>0.372822</td>\n",
       "      <td>0.836638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.267435</td>\n",
       "      <td>0.670167</td>\n",
       "      <td>0.382308</td>\n",
       "      <td>0.799162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.503673</td>\n",
       "      <td>0.268758</td>\n",
       "      <td>0.714185</td>\n",
       "      <td>0.390547</td>\n",
       "      <td>0.819068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9457' max='9457' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9457/9457 15:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuning(train_dataset,test_dataset,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79456504-acae-42c4-a992-dbcb148d41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### not in use\n",
    "# Define a function to compute the metrics\n",
    "def compute_metrics_evalaution(dataset, model, data_collator,prefix):\n",
    "    # Get the predictions using the trained model\n",
    "    trainer = Trainer(model=model, data_collator=data_collator)\n",
    "    predictions, labels, _ = trainer.predict(test_dataset=dataset)\n",
    "    \n",
    "    # Remove padding and special tokens\n",
    "    mask = labels != -100\n",
    "    true_labels = labels[mask].flatten()\n",
    "    flat_predictions = np.argmax(predictions, axis=2)[mask].flatten().tolist()\n",
    "\n",
    "    # Compute the metrics\n",
    "    accuracy = accuracy_score(true_labels, flat_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, flat_predictions, average='binary')\n",
    "    auc = roc_auc_score(true_labels, flat_predictions)\n",
    "    mcc = matthews_corrcoef(true_labels, flat_predictions)  # Compute the MCC\n",
    "    \n",
    "    return {prefix+\"accuracy\": accuracy, prefix+\"precision\": precision, prefix+\"recall\": recall, prefix+\"f1\": f1, prefix+\"auc\": auc, prefix+\"mcc\": mcc}  \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
